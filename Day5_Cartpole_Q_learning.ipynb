{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/meetAmarAtGithub/7_Reinforcement_Learning/blob/main/Day5_Cartpole_Q_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EInHjxT-tNO8"
      },
      "source": [
        "#|https://github.com/JackFurby/CartPole-v0/blob/master/cartPole.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        },
        "id": "pvLSRPCF_ZWG",
        "outputId": "46c34f61-5465-4fe4-8129-d6773d834aa9"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "env = gym.make('CartPole-v0')\n",
        "\n",
        "# How much new info will override old info. 0 means nothing is learned, 1 means only most recent is considered, old knowledge is discarded\n",
        "LEARNING_RATE = 0.1\n",
        "# Between 0 and 1, mesue of how much we carre about future reward over immedate reward\n",
        "DISCOUNT = 0.95\n",
        "RUNS = 10000  # Number of iterations run\n",
        "SHOW_EVERY = 2000  # How oftern the current solution is rendered\n",
        "UPDATE_EVERY = 100  # How oftern the current progress is recorded\n",
        "\n",
        "# Exploration settings\n",
        "epsilon = 1  # not a constant, going to be decayed\n",
        "START_EPSILON_DECAYING = 1\n",
        "END_EPSILON_DECAYING = RUNS // 2\n",
        "epsilon_decay_value = epsilon / (END_EPSILON_DECAYING - START_EPSILON_DECAYING)\n",
        "\n",
        "\n",
        "# Create bins and Q table\n",
        "def create_bins_and_q_table():\n",
        "\t# env.observation_space.high\n",
        "\t# [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n",
        "\t# env.observation_space.low\n",
        "\t# [-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]\n",
        "\n",
        "\t# remove hard coded Values when I know how to\n",
        "\n",
        "\tnumBins = 20\n",
        "\tobsSpaceSize = len(env.observation_space.high)\n",
        "\n",
        "\t# Get the size of each bucket\n",
        "\tbins = [\n",
        "\t\tnp.linspace(-4.8, 4.8, numBins),\n",
        "\t\tnp.linspace(-4, 4, numBins),\n",
        "\t\tnp.linspace(-.418, .418, numBins),\n",
        "\t\tnp.linspace(-4, 4, numBins)\n",
        "\t]\n",
        "\n",
        "\tqTable = np.random.uniform(low=-2, high=0, size=([numBins] * obsSpaceSize + [env.action_space.n]))\n",
        "\n",
        "\treturn bins, obsSpaceSize, qTable\n",
        "\n",
        "\n",
        "# Given a state of the enviroment, return its descreteState index in qTable\n",
        "def get_discrete_state(state, bins, obsSpaceSize):\n",
        "\tstateIndex = []\n",
        "\tfor i in range(obsSpaceSize):\n",
        "\t\tstateIndex.append(np.digitize(state[i], bins[i]) - 1) # -1 will turn bin into index\n",
        "\treturn tuple(stateIndex)\n",
        "\n",
        "\n",
        "bins, obsSpaceSize, qTable = create_bins_and_q_table()\n",
        "\n",
        "previousCnt = []  # array of all scores over runs\n",
        "metrics = {'ep': [], 'avg': [], 'min': [], 'max': []}  # metrics recorded for graph\n",
        "\n",
        "for run in range(RUNS):\n",
        "\tdiscreteState = get_discrete_state(env.reset(), bins, obsSpaceSize)\n",
        "\tdone = False  # has the enviroment finished?\n",
        "\tcnt = 0  # how may movements cart has made\n",
        "\n",
        "\twhile not done:\n",
        "\t\tif run % SHOW_EVERY == 0:\n",
        "\t\t\tenv.render()  # if running RL comment this out\n",
        "\n",
        "\t\tcnt += 1\n",
        "\t\t# Get action from Q table\n",
        "\t\tif np.random.random() > epsilon:\n",
        "\t\t\taction = np.argmax(qTable[discreteState])\n",
        "\t\t# Get random action\n",
        "\t\telse:\n",
        "\t\t\taction = np.random.randint(0, env.action_space.n)\n",
        "\t\tnewState, reward, done, _ = env.step(action)  # perform action on enviroment\n",
        "\n",
        "\t\tnewDiscreteState = get_discrete_state(newState, bins, obsSpaceSize)\n",
        "\n",
        "\t\tmaxFutureQ = np.max(qTable[newDiscreteState])  # estimate of optiomal future value\n",
        "\t\tcurrentQ = qTable[discreteState + (action, )]  # old value\n",
        "\n",
        "\t\t# pole fell over / went out of bounds, negative reward\n",
        "    # formula to caculate all Q values\n",
        "\t\tif done and cnt < 200:\n",
        "      newQ = (1 - LEARNING_RATE) * currentQ + LEARNING_RATE * (reward + DISCOUNT * maxFutureQ)\n",
        "      qTable[discreteState + (action, )] = newQ  # Update qTable with new Q value\n",
        "      discreteState = newDiscreteState\n",
        "      previousCnt.append(cnt)\n",
        "\n",
        "\t# Decaying is being done every run if run number is within decaying range\n",
        "\tif END_EPSILON_DECAYING >= run >= START_EPSILON_DECAYING:\n",
        "\t\tepsilon -= epsilon_decay_value\n",
        "\n",
        "\t# Add new metrics for graph\n",
        "\tif run % UPDATE_EVERY == 0:\n",
        "\t\tlatestRuns = previousCnt[-UPDATE_EVERY:]\n",
        "\t\taverageCnt = sum(latestRuns) / len(latestRuns)\n",
        "\t\tmetrics['ep'].append(run)\n",
        "\t\tmetrics['avg'].append(averageCnt)\n",
        "\t\tmetrics['min'].append(min(latestRuns))\n",
        "\t\tmetrics['max'].append(max(latestRuns))\n",
        "\t\tprint(\"Run:\", run, \"Average:\", averageCnt, \"Min:\", min(latestRuns), \"Max:\", max(latestRuns))\n",
        "\n",
        "\n",
        "env.close()\n",
        "\n",
        "# Plot graph\n",
        "plt.plot(metrics['ep'], metrics['avg'], label=\"average rewards\")\n",
        "plt.plot(metrics['ep'], metrics['min'], label=\"min rewards\")\n",
        "plt.plot(metrics['ep'], metrics['max'], label=\"max rewards\")\n",
        "plt.legend(loc=4)\n",
        "plt.show()  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-13f4467c580c>\"\u001b[0;36m, line \u001b[0;32m86\u001b[0m\n\u001b[0;31m    newQ = (1 - LEARNING_RATE) * currentQ + LEARNING_RATE * (reward + DISCOUNT * maxFutureQ)\u001b[0m\n\u001b[0m                                                                                            ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fwUGpCjDXSF"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}