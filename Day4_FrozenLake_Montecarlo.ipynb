{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/meetAmarAtGithub/7_Reinforcement_Learning/blob/main/Day4_FrozenLake_Montecarlo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c58b100",
      "metadata": {
        "id": "9c58b100",
        "outputId": "986ad7e0-2090-4f45-eb9c-4f92665db85a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{(0, 0): {'D': 0, 'R': 0}, (0, 1): {'L': 0, 'D': 0, 'R': 0}, (0, 2): {'L': 0, 'D': 0, 'R': 0}, (0, 3): {'L': 0, 'D': 0}, (1, 0): {'U': 0, 'D': 0, 'R': 0}, (1, 1): {'U': 0, 'L': 0, 'D': 0, 'R': 0}, (1, 2): {'U': 0, 'L': 0, 'D': 0, 'R': 0}, (1, 3): {'U': 0, 'L': 0, 'D': 0}, (2, 0): {'U': 0, 'D': 0, 'R': 0}, (2, 1): {'U': 0, 'L': 0, 'D': 0, 'R': 0}, (2, 2): {'U': 0, 'L': 0, 'D': 0, 'R': 0}, (2, 3): {'U': 0, 'L': 0, 'D': 0}, (3, 0): {'U': 0, 'R': 0}, (3, 1): {'U': 0, 'L': 0, 'R': 0}, (3, 2): {'U': 0, 'L': 0, 'R': 0}}\n",
            " | R |  | L |  | L |  | D | \n",
            "----------------------------\n",
            " | D |  | U |  | L |  | L | \n",
            "----------------------------\n",
            " | U |  | R |  | R |  | L | \n",
            "----------------------------\n",
            " | U |  | L |  | L | \n",
            "----------------------------\n",
            "\n",
            "\n",
            "\n",
            " step:0\n",
            " | R |  | L |  | L |  | L | \n",
            "----------------------------\n",
            " | U |  | U |  | U |  | U | \n",
            "----------------------------\n",
            " | U |  | U |  | U |  | U | \n",
            "----------------------------\n",
            " | U |  | U |  | U | \n",
            "----------------------------\n",
            "\n",
            "\n",
            "\n",
            " step:100\n",
            " | R |  | D |  | L |  | L | \n",
            "----------------------------\n",
            " | R |  | R |  | D |  | L | \n",
            "----------------------------\n",
            " | D |  | R |  | R |  | D | \n",
            "----------------------------\n",
            " | R |  | R |  | R | \n",
            "----------------------------\n",
            "\n",
            "\n",
            "\n",
            " step:200\n",
            " | R |  | D |  | D |  | L | \n",
            "----------------------------\n",
            " | R |  | R |  | D |  | L | \n",
            "----------------------------\n",
            " | R |  | R |  | R |  | D | \n",
            "----------------------------\n",
            " | R |  | R |  | R | \n",
            "----------------------------\n",
            "\n",
            "\n",
            "\n",
            " step:300\n",
            " | R |  | D |  | D |  | L | \n",
            "----------------------------\n",
            " | D |  | R |  | D |  | L | \n",
            "----------------------------\n",
            " | R |  | R |  | D |  | D | \n",
            "----------------------------\n",
            " | R |  | R |  | R | \n",
            "----------------------------\n",
            "\n",
            "\n",
            "\n",
            " step:400\n",
            " | R |  | D |  | L |  | L | \n",
            "----------------------------\n",
            " | D |  | R |  | D |  | L | \n",
            "----------------------------\n",
            " | D |  | R |  | D |  | D | \n",
            "----------------------------\n",
            " | R |  | R |  | R | \n",
            "----------------------------\n",
            "\n",
            "\n",
            "\n",
            " step:500\n",
            " | R |  | D |  | D |  | L | \n",
            "----------------------------\n",
            " | D |  | R |  | D |  | D | \n",
            "----------------------------\n",
            " | R |  | R |  | D |  | D | \n",
            "----------------------------\n",
            " | R |  | R |  | R | \n",
            "----------------------------\n",
            "\n",
            "\n",
            "\n",
            " step:600\n",
            " | R |  | D |  | L |  | L | \n",
            "----------------------------\n",
            " | D |  | R |  | D |  | D | \n",
            "----------------------------\n",
            " | R |  | R |  | R |  | D | \n",
            "----------------------------\n",
            " | R |  | R |  | R | \n",
            "----------------------------\n",
            "\n",
            "\n",
            "\n",
            " step:700\n",
            " | D |  | R |  | D |  | L | \n",
            "----------------------------\n",
            " | R |  | U |  | D |  | D | \n",
            "----------------------------\n",
            " | R |  | R |  | R |  | D | \n",
            "----------------------------\n",
            " | R |  | R |  | R | \n",
            "----------------------------\n",
            "\n",
            "\n",
            "\n",
            " step:800\n",
            " | R |  | R |  | D |  | D | \n",
            "----------------------------\n",
            " | D |  | L |  | D |  | D | \n",
            "----------------------------\n",
            " | D |  | R |  | D |  | D | \n",
            "----------------------------\n",
            " | R |  | R |  | R | \n",
            "----------------------------\n",
            "\n",
            "\n",
            "\n",
            " step:900\n",
            " | R |  | R |  | D |  | D | \n",
            "----------------------------\n",
            " | R |  | R |  | D |  | D | \n",
            "----------------------------\n",
            " | D |  | R |  | D |  | D | \n",
            "----------------------------\n",
            " | R |  | R |  | R | \n",
            "----------------------------\n",
            "\n",
            "\n",
            "\n",
            " step:1000\n",
            " | D |  | R |  | D |  | D | \n",
            "----------------------------\n",
            " | R |  | U |  | D |  | D | \n",
            "----------------------------\n",
            " | R |  | R |  | D |  | D | \n",
            "----------------------------\n",
            " | U |  | R |  | R | \n",
            "----------------------------\n",
            "exploited:6184618  explored:325420\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import copy\n",
        "class GridWorld:\n",
        "    def __init__(self):\n",
        "        # S O O O\n",
        "        # O O O *\n",
        "        # O * O O\n",
        "        # O * 0 T\n",
        "        self.qTable = None\n",
        "        self.actionSpace = ('U', 'D', 'L', 'R')\n",
        "        self.actions = {\n",
        "            (0, 0): ('D', 'R'),\n",
        "            (0, 1): ('L', 'D', 'R'),\n",
        "            (0, 2): ('L', 'D', 'R'),\n",
        "            (0, 3): ('L', 'D'),\n",
        "            (1, 0): ('U', 'D', 'R'),\n",
        "            (1, 1): ('U', 'L', 'D', 'R'),\n",
        "            (1, 2): ('U', 'L', 'D', 'R'),\n",
        "            (1, 3): ('U', 'L', 'D'),\n",
        "            (2, 0): ('U', 'D', 'R'),\n",
        "            (2, 1): ('U', 'L', 'D', 'R'),\n",
        "            (2, 2): ('U', 'L', 'D', 'R'),\n",
        "            (2, 3): ('U', 'L', 'D'),\n",
        "            (3, 0): ('U', 'R'),\n",
        "            (3, 1): ('U', 'L', 'R'),\n",
        "            (3, 2): ('U', 'L', 'R')\n",
        "        }\n",
        "        self.rewards = {(3, 3): 0.03, (1, 3): -0.01, (2, 1):-0.011, (3, 1):-0.01}\n",
        "        self.explored = 0\n",
        "        self.exploited = 0\n",
        "        self.initialQtable()\n",
        "\n",
        "    def initialQtable(self):\n",
        "      self.qTable = {}\n",
        "      for state in self.actions:\n",
        "          self.qTable[state]={}\n",
        "          for move in self.actions[state]:\n",
        "              self.qTable[state][move]=0\n",
        "      print(self.qTable)\n",
        "\n",
        "    def updateQtable(self, newQ,updateRate=0.05):\n",
        "        for state in self.qTable:\n",
        "            for action in self.qTable[state]:\n",
        "                self.qTable[state][action] = self.qTable[state][action]+(updateRate*(newQ[state][action]-self.qTable[state][action]))\n",
        "   \n",
        "    def getRandomPolicy(self):\n",
        "        policy = {}\n",
        "        for state in self.actions:\n",
        "            policy[state] = np.random.choice(self.actions[state])\n",
        "        return policy\n",
        "\n",
        "    def reset(self):\n",
        "        return (0, 0)\n",
        "        \n",
        "    def is_terminal(self, s):\n",
        "        return s not in self.actions\n",
        "\n",
        "    def getNewState(self,state,action):\n",
        "      i, j = zip(state)\n",
        "      row = int(i[0])\n",
        "      column = int(j[0])\n",
        "      if action == 'U':\n",
        "          row -= 1\n",
        "      elif action == 'D':\n",
        "          row += 1\n",
        "      elif action == 'L':\n",
        "          column -= 1\n",
        "      elif action == 'R':\n",
        "          column += 1\n",
        "      return row,column\n",
        "\n",
        "    def chooseAction(self, state, policy, exploreRate):\n",
        "        if exploreRate > np.random.rand():\n",
        "            self.explored += 1\n",
        "            return np.random.choice(self.actions[state])\n",
        "        self.exploited += 1\n",
        "        return policy[state]\n",
        "\n",
        "    def move(self, state, policy, exploreRate):\n",
        "        action = self.chooseAction(state, policy, exploreRate)\n",
        "        row,column=self.getNewState(state,action)\n",
        "        if (row, column) in self.rewards:\n",
        "            return action,(row, column),self.rewards[(row, column)]\n",
        "        return action,(row, column), 0\n",
        "        \n",
        "    def printPolicy(self, policy):\n",
        "        line = \"\"\n",
        "        counter = 0\n",
        "        for item in policy:\n",
        "            line += f\" | {policy[item]} | \"\n",
        "            counter += 1\n",
        "            if counter > 3:\n",
        "                print(line)\n",
        "                print(\"----------------------------\")\n",
        "                counter = 0\n",
        "                line = \"\"\n",
        "        print(line)\n",
        "        print(\"----------------------------\")\n",
        "        \n",
        "\n",
        "enviroment = GridWorld()\n",
        "policy = enviroment.getRandomPolicy()\n",
        "enviroment.printPolicy(policy)\n",
        "\n",
        "# example optimal policy = {(0, 0): 'R', (0, 1): 'R', (0, 2): 'D', (0, 3): 'D', (1, 0): 'R', (1, 1): 'D', (1, 2): 'D', (1, 3): 'D',\n",
        "#           (2, 0): 'R', (2, 1): 'D', (2, 2): 'R', (2, 3): 'D', (3, 0): 'R', (3, 1): 'R', (3, 2): 'R'}\n",
        "\n",
        "for i in range(1001):\n",
        "  estimatedQ = copy.deepcopy(enviroment.qTable)\n",
        "  collectedSampls = 0\n",
        "  for j in range(1000):\n",
        "    trajectory = []\n",
        "    state = enviroment.reset()\n",
        "    stepCounts=0\n",
        "\n",
        "    while (not enviroment.is_terminal(state)) and (stepCounts<30):\n",
        "      action,nextState, reward = enviroment.move(state, policy, exploreRate=0.05)\n",
        "      trajectory.append(((state, action), reward))\n",
        "      state=nextState\n",
        "      stepCounts+=1\n",
        "    collectedSampls += 1\n",
        "    rewards=0\n",
        "    for item in reversed(trajectory):\n",
        "            q,reward=zip(item)\n",
        "            rewards +=0.9*(reward[0])\n",
        "            estimatedQ[q[0][0]][q[0][1]] = estimatedQ[q[0][0]][q[0][1]] + ((1 / collectedSampls) * (rewards - estimatedQ[q[0][0]][q[0][1]]))\n",
        "    enviroment.updateQtable(estimatedQ)\n",
        "    for state in policy:\n",
        "        policy[state] = max(enviroment.qTable[state], key=enviroment.qTable[state].get)\n",
        "  if (i%100)==0:\n",
        "    print(f\"\\n\\n\\n step:{i}\")\n",
        "    enviroment.printPolicy(policy)\n",
        "\n",
        "print(f\"exploited:{enviroment.exploited}  explored:{enviroment.explored}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01e655db",
      "metadata": {
        "id": "01e655db"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}